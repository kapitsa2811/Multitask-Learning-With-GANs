1. Initializations make a significant difference. How many different seeds should be used when a result is reported? 1, 3, 5?

2. Often my models take very long to converge, especially with MTL. To get them to converge quickly, the models must be simplified drastically. This gives terrible results. Is this a compromise worth making?    

3. How much exploration into MTL classification should I do?

4. How many epochs should I run the model comparisons for when doing MTL vs non-MTL? Should the MTL models be run for twice the epochs because each task gets ran 50% of the time? Ideally they'd be run till convergence, right? 

5. Is it a fair comparison, MTL against non-MTL, if the learning rate is getting halved for the MTL tasks? How do we know the difference in performance isn't the different learning rates? 

6. General questions about what level to pitch the diss at. Especially the background info. 
