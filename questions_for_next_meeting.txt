1. Could batchnorm pose a problem for MTL? My models use a standard tf.contrib.layers.batch_norm implemention 

2. How widespread is ELU? After reading the paper it looked like it should often outperform RELU. When I tried it on CIFAR I got bad results.

3. Initializations make a significant difference. How many different seeds should be used when a result is reported? 1, 3, 5?

4. Often my models take very long to converge, especially with MTL. To get them to converge quickly (without BN), the models must be simplified drastically. This gives terrible results. Is this a compromise worth making?    

5. How much exploration into MTL classification should I do?

6. How many epochs should I run the model comparisons for when doing MTL vs non-MTL? Should the MTL models be run for twice the epochs because each task gets ran 50% of the time? Ideally they'd be run till convergence, right? 

7. Is it a fair comparison, MTL against non-MTL, if the learning rate is getting halved for the MTL tasks? How do we know the difference in performance isn't the different learning rates? 

